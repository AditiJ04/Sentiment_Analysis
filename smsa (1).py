# -*- coding: utf-8 -*-
"""SMSA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wfmh0jTCM39aLwk5x4wiR9jKAK1lxFOy
"""

#installing kaggle library
! pip install kaggle
#Kaggle is the platform providing us the dataset.

# configuring the path of Kaggle.json file
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
#copy pasted this from the video

# API to fetch the datatset from Kaggle
!kaggle datasets download -d kazanova/sentiment140
#copy pasted the above line from Kaggle -> Copy API Command.
#we used api because downloading all of the data set would take time

# extracting the compressed dataset

from zipfile import ZipFile
dataset = '/content/sentiment140.zip'

with ZipFile(dataset, 'r') as zip:
  #the zip is a variable here, r is the read mode that we are opening the above file in read mode
  zip.extractall()
  #once all the data has been read/ extracted the program will print the following statement
  print('The dataset is extracted')

"""Importing the dependencies."""

#importing the dependencies and functions for the process
import numpy as np
#it is not easy to read data from the csv file like the file we downloaded so we import pandas that convert it into dataframes (structured frames)
import pandas as pd
import re
#re stands for regular expression and we import it for pattern matching/searching, etc.
#since we are using google colab we do not require to sepcially download the libraries using pip install
from nltk.corpus import stopwords
#nltk stands for natural language toolkit and corpus is the module we will be using. And from the corpus we will be using the stopwords module.
from nltk.stem.porter import PorterStemmer
#PorterStemmer is used for the Stemming that we do i.e. searching the root words.
from sklearn.feature_extraction.text import TfidfVectorizer
#include sklearn is a widely used machine learning library.TfidfVectorizer module is used to convert textural data into numerical data as we cannot make use of textual data.
from sklearn.model_selection import train_test_split
#train_test is used to split our original data into training test data where the training data will be further used to train our machine learning model and evaluating our model.
from sklearn.linear_model import LogisticRegression
#importing machine learning model and we are using LogisticRegression
from sklearn.metrics import accuracy_score
#importing accuracy score

#downloading stopwords
import nltk
nltk.download('stopwords')

#printing the stopwords in english
print(stopwords.words('english'))
#stopwords are the words that dont add any meaning to the text or data or words that dont contribute much contextual importance to the sentence and that is why we are removing them
#since we have a very large data set and inorder to reduce the time complexity to realise and elavulate on that data we are removing stopwords form the data

"""DATA PROCESSING"""

#loading the data from csv file through pandas dataframe
twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv',encoding='ISO-8859-1')
#pd.read_csv will read the content from the csv file and load it into pandas dataframe

# checking the number of rows and columns
#number of rows is the number of tweets we have
twitter_data.shape
#shape function will return how many rows and columns we have in our dataset

#printing the first five rows of the dataframe
twitter_data.head()
#the column names i.e. - target,id,text..etc are not read by the pd.read_csv() and that explains the following output

#naming the columns and reading the dataset again

column_names=['target','id','date','flag','user','text']
twitter_data = pd.read_csv('/content/training.1600000.processed.noemoticon.csv',names=column_names ,encoding='ISO-8859-1')

twitter_data.shape

twitter_data.head()
#in the output bellow the 0 represents negative tweet and the 4 represents positive tweet

#checking if there are any missing values
#counting the number of missing values (text in this case) in the dataset
twitter_data.isnull().sum()
#the follwing outout confirms that there are no missing values in the dataset as the count comes out to be zero

#checking (how many labels are there i.e number of positive and negative tweets there are in the dataset) the distribution of target column
twitter_data['target'].value_counts()
#if the distribution is not balanced/even then we have to do upsampling aand downsampling as without the even distribution the machine learning model wont work properly

"""Convert the target "4" to "1"

"""

#converting 4 to 1 so that 0 means negative tweets and 1 means positive tweet
twitter_data.replace({'target':{4:1}}, inplace=True)
#inplace = True means the changes we are making we wish them to be stored or reflect on the original data

twitter_data['target'].value_counts()
# all the 4 has been converted into 1

"""0 means negative tweet and 1 means postive tweet

STEMMING :

Its the process of reducing a  word to its Root Word. For eg: word=actress,acting,actor and root word= act.
"""

#Stemming is a process of reducing a word into its root word
port_stem = PorterStemmer()
#the above statement will load the instance of PorterStemmer to variable port_stem

#Creating a function
def stemming(content):
  #the parameter content here represents the tweets/text we will be passing to this function
  stemmed_content = re.sub('[^a-zA-Z]',' ',content)
  #now we will remove('^' -> means removing) all the data that does not belomg to alphabets from a to z and A to Z,  for eg: special symbols,punctuators and digits is whaat we want to eliminate.
  #For eg in the first tweet we encounter "@" so we would want to get rid of it

  stemmed_content = stemmed_content.lower()
  #converting all letters to lowercase

  stemmed_content =stemmed_content.split()
  #splitting all the tweets in the twwet and we are putting them on a list

  stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]
  #stemmed_content stores all the processed data and now we reduce the words to their root words using port_stem and if does not exists in stopwords.

  stemmed_content= ' '.join(stemmed_content)
  #joining the content of the tweet back together into a single tweet.

  return stemmed_content

twitter_data['stemmed_content'] = twitter_data['text'].apply(stemming)

import tensorflow as tf
tf.keras.Model.save

#separating the data and label
X = twitter_data['stemmed_content'].values
Y = twitter_data['target'].values

print(X)
#stemmed content

print(Y)
#spilliting the data into test data and training data

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, stratify=Y, random_state=2)
#X_train will contain all the trainig data tweets and Y_train is the target the label for my traning data
#X_test contains test data tweets and Y_test contains whether its a positive tweet or negative tweet
#train_test_split is the fucntion we use to split x and y so x is my tweet and y is my label
#test size means how many data points so 0.2 means 20% of my 1.6 million tweeets data will go to my test data and other 80% is training data
#statify means is I want almost an equal distribution of zero and one so this means that training data if proportion of positive Tweets and the negative tweets is like 50/50.
#statify=Y means that we have to tell train test plate that i want equal proportion of the two classes which are 0 and 1 , both in my training data as well as test data.

#random split means without this parameter each time we split data it will split randomly.

print(X.shape, X_train.shape, X_test.shape)

print(X_train)

print(X_test)

#converting the textual data to numerical data

vectorizer= TfidfVectorizer()

#TfidfVectorizer will convert all words into numerical values based on its importance(impirtance may be based on frquency of the word)

X_train= vectorizer.fit_transform(X_train)
#the .fit_transform is used because the data has to be fitted that is the ml model needs to realise what kind of data it is
#transforming and fitting the training data that means that the training data will be taken and and all words will be assigned importance and based on this transformation the test data will be transformed

X_test= vectorizer.transform(X_test)

print(X_train)

print(X_test)

#Training the machine learning model

"""Logistic Regression"""

model = LogisticRegression(max_iter=1000)

#max_iter means the maximum no of times the model has to go through the data

model.fit(X_train, Y_train) #model is learning from the data
#X_train= training data tweets and Y_train means target data tweets

#The model will try to map the 1 to positive tweets and 0 to the negative tweets

"""Model Evaluation

Accuracy Score
"""

#Accuracy score on the training data

X_train_prediction = model.predict(X_train)
training_data_accuracy = accuracy_score(Y_train,X_train_prediction)
#comparing true values(Y_train) and the values predicted by our model (X_train_prediction)

print('Accuracy score on the training data:',training_data_accuracy)
#comes out to be 81%

#Accuracy score on the test data

X_test_prediction = model.predict(X_test)
test_data_accuracy = accuracy_score(Y_test,X_test_prediction)
#comparing true values(Y_test) and the values predicted by our model (X_test_prediction)

print('Accuracy score on the test data:',test_data_accuracy)
#comes out to be 77%

#since the training and test accuracy are pretty close that means our model is working perfectly.

"""Model Accuracy =77.8%

Saving the trained model
"""

import pickle
filename='trained_model.sav'
pickle.dump(model,open(filename,'wb')) #write into a new filee in binary format

"""Using the saved model for future predictions"""

#loading the saved model
loaded_model = pickle.load(open('trained_model.sav','rb'))

X_new = X_test[200]
print(Y_test[200])

prediction = model.predict(X_new)
print(prediction)

if(prediction[0]==0):
  print('The tweet is negative')
else:
  print('The tweet is positive')

